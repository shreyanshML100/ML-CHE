# -*- coding: utf-8 -*-
"""Linear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_raI1RZ4BGMcZsmytNlwDabUHZRoEMVK
"""

import numpy as np
import matplotlib.pyplot as plt

def costFun(Theta,x_train,y_train):
    s=np.sum((np.matmul(Theta,x_train)-y_train)**2);
    return (1/(2*y_train.shape[1]))*s;

def diffCostFun(Theta,x_train,y_train):
    b=0;
    b=np.dot((np.matmul(Theta,x_train)-y_train),np.transpose(x_train));
    b=b/(y_train.size);
    return b;
  
def fitGD(x_train,y_train,alpha,lamda,Type,iterations,pt):
  Theta=np.array([1.,1.]);
  m=y_train.shape[1];
  cost=np.zeros(iterations);
  
  for itr in range(iterations):
    if Type==1:
      r1=0.;
      r2=np.zeros(2);
      r1=(np.sum(np.absolute(Theta)))*lamda/(2*m);
      r2=(lamda/(2*m));
      k=diffCostFun(Theta,x_train,y_train)+r2;
      Theta=Theta-alpha*k;
   
    if Type==2:
      r1=0;
      r2=np.zeros(2);
      r1=np.sum(np.square(Theta))*lamda/m;
      r2=(lamda/m)*(Theta);
      k=diffCostFun(Theta,x_train,y_train)+r2;
      Theta=Theta-alpha*k;

    if Type==3:
      r1=0;
      r1=0.5*(np.square(Theta)+np.absolute(Theta));
      r2=np.zeros(2);
      r2=(lamda/m)*(0.5*(Theta+0.5*np.ones(Theta.size)));
      k=diffCostFun(Theta,x_train,y_train)+r2;
      Theta=Theta-alpha*k;
    cost[itr]=(costFun(Theta,x_train,y_train)+r1);

  if pt==1:
    plt.plot(cost)
    plt.show()
    plt.scatter(x_train[1],y_train);
    x = np.linspace(0,12,100)
    y=Theta[0,1]*x+Theta[0,0];
    plt.plot(x,y,"r-");
    plt.show()
  return Theta;

def fitNormal(x_train,y_train):
  yn= np.transpose(y_train);
  xn=np.transpose(x_train);
  return np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(xn),xn)),np.transpose(xn)),yn)

def locallyWeighted(x_train,y_train,x_test,tau,iterations):
  xn=np.transpose(x_train);
  w=np.mat(np.identity(x_train.shape[1],dtype=float));
  y=np.array([]);
  theta=np.transpose(np.zeros(2,dtype=float));
  for j in range(x_test.shape[0]):
    for i in range(xn.shape[0]):
      w[i,i]=np.exp(-1*((xn[i,1]-x_test[j,1])**2)/(2*tau*tau));
    theta=np.matmul(np.linalg.inv(np.matmul(np.transpose(xn),np.matmul(w,xn))),np.matmul(np.transpose(xn),np.matmul(w,np.transpose(y_train))));
    y=np.append(y,np.dot(x_test[j],theta));
  return y;

def error(x_train,y_train,x_test,y_test):
  j_train=np.zeros(16);
  j_test=np.zeros(16);
  for l in range(16):
    theta=fitGD(x_train,y_train,0.01,l,1,100,0);
    y_ptrain=np.matmul(theta,x_train);
    j_train[l]=np.sum((y_train-y_ptrain)**2)/(2*y_train.shape[1]);
    theta=fitGD(x_train,y_train,0.01,l,1,100,0);
    y_ptest=np.matmul(theta,np.transpose(x_test));
    j_test[l]=np.sum((y_test-y_ptest)**2)/(2*y_test.shape[1]);
  plt.plot(j_train,color="red");
  plt.plot(j_test,color="blue");
  plt.show();
  tau=1;
  j_train=np.zeros(5);
  j_test=np.zeros(5);
  t=np.array([1,10,100,1000,10000]);
  l=0;
  while tau<=10000:
    xnn=np.transpose(x_train);
    y_ptrain=locallyWeighted(x_train,y_train,xnn,tau,100);
    j_train[l]=np.sum((y_train-y_ptrain)**2)/(2*y_train.shape[1]);
    y_ptest=locallyWeighted(x_train,y_train,x_test,tau,100);
    j_test[l]=np.sum((y_train-y_ptrain)**2)/(2*y_train.shape[1]);
    tau=tau*10;
    l=l+1;
  plt.plot(t,j_train,"r-");
  plt.plot(t,j_test,color="blue");
  plt.title("here, the blue and red lines have overlapped");
  plt.show()

  return;