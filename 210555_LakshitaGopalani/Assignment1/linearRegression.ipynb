{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjINZlg2y_vJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def costFun(theta, X_train, Y_train):\n",
        "    m = X_train.shape[0]\n",
        "    pred = X_train@theta\n",
        "    J = np.linalg.norm(pred - Y_train)**2 / (2*m)\n",
        "    return J"
      ],
      "metadata": {
        "id": "qE8_PxC4zHTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diffCostFun(X_train, Y_train, theta):\n",
        "  m = X_train.shape[0]\n",
        "  return (1/m) * (((X_train.T @ X_train) @ theta) - (X_train.T @ Y_train)) \n",
        "  "
      ],
      "metadata": {
        "id": "bL4d4d53zQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitGD(X_train, Y_train, alpha, l, reg, it):\n",
        "  m = X_train.shape[0]\n",
        "  n = X_train.shape[1] - 1 \n",
        "  theta = np.zeros((n+1, 1))\n",
        "  grad_abs_theta = np.ones((n+1, 1))\n",
        "  L1 = []\n",
        "  L2 = []\n",
        "  L3 = []\n",
        "  \n",
        "  for i in range(it):\n",
        "    L1.append(costFun(theta,X_train,Y_train) + (l/n)*(np.linalg.norm(theta,1)))\n",
        "    grad_abs_theta[theta < 0] = -1\n",
        "    grad_L1 = diffCostFun(X_train, Y_train, theta) + (l/n) * (grad_abs_theta)\n",
        "    \n",
        "    L2.append(costFun(theta,X_train,Y_train) + (l/n)*(np.linalg.norm(theta,2))) \n",
        "    grad_L2 = diffCostFun(X_train, Y_train, theta) + (l/n) * (theta)\n",
        "\n",
        "    L3.append((L1[i] + L2[i])/2)  #elastic net with equal weight for L1 and L2 (mixing coefficient = 0.5)\n",
        "    grad_L3 = (grad_L1 + grad_L2) / 2\n",
        "    \n",
        "    if reg == 1:\n",
        "      theta = theta - (alpha * grad_L1)\n",
        "    elif reg == 2:\n",
        "      theta = theta - (alpha * grad_L2)\n",
        "    elif reg == 3:\n",
        "      theta = theta - (alpha * grad_L3)\n",
        "\n",
        "  iter = [i for i in range(1,it+1)]\n",
        "  if(reg==1):\n",
        "    plt.gca()\n",
        "    plt.plot(iter, L1, label='Gradient Descent with L1 Regularisation')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost Function')\n",
        "    plt.title('Variation of Cost Function with Iterations')\n",
        "    plt.legend()\n",
        "    plt.show\n",
        "  if(reg==2):\n",
        "    plt.gca()\n",
        "    plt.plot(iter,L2, label='Gradient Descent with L2 Regularisation')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost Function')\n",
        "    plt.title('Variation of Cost Function with Iterations')\n",
        "    plt.legend()\n",
        "    plt.show\n",
        "  if(reg==3):\n",
        "    plt.gca()\n",
        "    plt.plot(iter,L3label='Gradient Descent with Elastic Net Regularisation')  \n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost Function')\n",
        "    plt.title('Variation of Cost Function with Iterations')\n",
        "    plt.legend()\n",
        "    plt.show\n",
        "\n",
        "  return theta"
      ],
      "metadata": {
        "id": "lZrwT_MfzQ4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitNormal(X_train,Y_train):\n",
        "  theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ Y_train\n",
        "  return theta"
      ],
      "metadata": {
        "id": "NzmcgDQ3zRNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def locallyWeighted(X_train, Y_train, x, tau, alpha, it):\n",
        "  m = X_train.shape[0]\n",
        "  n = X_train.shape[1] - 1 \n",
        "  \n",
        "  pred = np.zeros((x.shape[0]))\n",
        "  pred_ = np.zeros((x.shape[0]))\n",
        "  \n",
        "  for i in range(x.shape[0]):\n",
        "    point = x[i]\n",
        "    w = np.zeros((m,m))\n",
        "    theta = np.zeros((n+1,1))\n",
        "    for j in range(m):\n",
        "      temp = -(X_train[j]-point) @ (X_train[j]-point).T\n",
        "      w[j][j] = np.exp(temp/(2 * (tau**2)))\n",
        "    for k in range(it):\n",
        "      # J = (1/(2*m)) * (((X_train@theta) - Y_train).T )@w@(X_train@theta - Y_train)\n",
        "      grad_J = (1/m) * (X_train.T @ w @ ((X_train @ theta) - Y_train))\n",
        "      theta = theta - (alpha* grad_J)\n",
        "    pred[i] = point @ theta\n",
        "\n",
        "  return pred"
      ],
      "metadata": {
        "id": "5yrIN3oKS4EZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}